{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Untitled5.ipynb","provenance":[],"mount_file_id":"1sLo1Fh9IkqQ7OBHzHLqicfi1Iu-D7G4D","authorship_tag":"ABX9TyMKpm06duhrXJWOHsfGClyg"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Oj6dl6vFyg_7"},"source":["import numpy as np\n","import cv2\n","from skimage.measure import label, regionprops\n","\n","\n","class Preprocess:\n","    def __init__(self, rawim, im, breast_mask, lesion_mask):\n","        self.raw = rawim\n","        self.image = im\n","        self.mask = breast_mask\n","        self.lesion_mask = lesion_mask\n","   \n","    def extract_breast_profile(image,lesion_mask, if_crop):\n","        \n","        breast_mask = np.zeros(np.shape(image))\n","        breast_mask[image>0]=1\n","        \n","        labelim = label(breast_mask)\n","        props =  regionprops(labelim)\n","#        find the largest object as the breast\n","        area = 0\n","        ind = 1\n","        for i in range(0,len(props)):\n","            if area<props[i].filled_area:\n","                area = props[i].filled_area\n","                ind = i+1\n","        breast_mask = np.zeros(np.shape(image))\n","        breast_mask[labelim==ind]=1  \n","        labelim = label(breast_mask)       \n","        props =  regionprops(labelim)\n","        boundingbox = props[0].bbox\n","#        crop the breast mask and mammogram\n","        if if_crop == 1:\n","            breast_mask = breast_mask[boundingbox[0]:boundingbox[2],boundingbox[1]:boundingbox[3]]\n","            breast_raw_image = image[boundingbox[0]:boundingbox[2],boundingbox[1]:boundingbox[3]]\n","            lesion_mask = lesion_mask[boundingbox[0]:boundingbox[2],boundingbox[1]:boundingbox[3]]\n","        else:\n","            breast_raw_image = image\n","#        breast_image = rescale2uint8(breast_raw_image,breast_mask)\n","        breast_image = rescale2uint16(breast_raw_image,breast_mask)\n","        return Preprocess(breast_raw_image,breast_image,breast_mask,lesion_mask)\n","    \n","def rescale2uint8(image,breast_mask):\n","    intensity_in_mask = image[breast_mask>0]\n","#    use top 0.2 percentile to do the strech\n","    maxi = np.percentile(intensity_in_mask,99.8)#np.max(intensity_in_mask)\n","    mini = np.percentile(intensity_in_mask,0.2)#np.min(intensity_in_mask)\n","#        stretch the image into 0~255\n","    \n","    image = 255*(image-mini)/(maxi-mini)\n","    image[breast_mask==0] = 0\n","    image[image<0] = 0\n","    image[image>255] = 255\n","    image = np.uint8(image)\n","          \n","    return image\n","\n","def rescale2uint16(image,breast_mask):\n","    intensity_in_mask = image[breast_mask>0]\n","#    use top 0.2 percentile to do the strech\n","    maxi = np.percentile(intensity_in_mask,99.8)#np.max(intensity_in_mask)\n","    mini = np.percentile(intensity_in_mask,0.2)#np.min(intensity_in_mask)\n","#        stretch the image into 0~255\n","    \n","    image = 65535*(image-mini)/(maxi-mini)\n","    image[breast_mask==0] = 0\n","    image[image<0] = 0\n","    image[image>65535] = 65535\n","    image = np.uint16(image)\n","          \n","    return image\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ceq3ws4Syh0p"},"source":["import numpy as np\n","# To pad image into a square shape\n","def padimages(image,file_name, ratio):\n","    [length, width] = np.shape(image)\n","    if length/width>ratio:#1024/800\n","        print('This image needs padding.')\n","        add_wid = round(length*(1/ratio)-width)\n","        pad = np.zeros((length,add_wid))\n","        pad = pad.astype(image.dtype)\n","        if '_R_' in file_name:\n","        #                pad on the left\n","            pad_image = np.concatenate((pad,image),axis=1)\n","        else:\n","            pad_image = np.concatenate((image,pad),axis=1)\n","            \n","    return pad_image"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_AkT0hjmyh3X"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from skimage import data, color, io, img_as_float\n","\n","def mask_overlay(img, mask):\n","    alpha = 0.5\n","\n","    img = img_as_float(img)\n","    rows, cols = img.shape\n","    # Construct RGB version of grey-level image\n","    img_color = np.dstack((img, img, img))\n","    img_hsv = color.rgb2hsv(img_color)\n","    color_mask = np.zeros((rows, cols, 3))\n","    color_mask1 = color_mask[:,:,1]\n","    color_mask1[np.where(mask>0)] = 1\n","    color_mask[:,:,1] = color_mask1\n","    color_mask_hsv = color.rgb2hsv(color_mask)\n","    \n","    # Replace the hue and saturation of the original image\n","    # with that of the color mask\n","    img_hsv[..., 0] = color_mask_hsv[..., 0]\n","    img_hsv[..., 1] = color_mask_hsv[..., 1] * alpha\n","    \n","    img_masked = color.hsv2rgb(img_hsv)\n","    \n","    return img_masked"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5gyj5Uz3ysJ7","executionInfo":{"status":"ok","timestamp":1617821636416,"user_tz":-360,"elapsed":744,"user":{"displayName":"Jasmin Jahan Puspo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrKQVqe7wC9jSaf9_-51xdh1f0mivy2S9SdHYt=s64","userId":"07570020587767099311"}},"outputId":"949e1345-9885-4c08-f078-7a087d2cbade"},"source":["cd '/content/drive/MyDrive/Mammographic/Mask_r_cnn'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Mammographic/Mask_r_cnn\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"guoFw9nKysNi"},"source":["# -*- coding: utf-8 -*-\n","\"\"\"\n","Created on Mon Nov 26 15:46:32 2018\n","\n","@author: Hang Min\n","\n","Prepare the mammograms for pseudo-color transformation\n","\n","To normalize the INbreast mammogram to 16bit\n","\n","To pad the mammograms and masks to a square\n","\n","\n","\"\"\"\n","\n","\n","\n","import os\n","import numpy as np\n","#from Preprocess_mammo import Preprocess \n","import matplotlib.pyplot as plt\n","from matplotlib.pyplot import figure\n","#from utility import padimages\n","#from skimage.segmentation import mark_boundaries\n","from skimage import io\n","from skimage.measure import label\n","#from overlay import mask_overlay\n","import timeit\n","\n","start = timeit.default_timer()\n","\n","import warnings\n","warnings.filterwarnings(\"ignore\")\n","\n","#from skimage import data, color, io, img_as_float\n","\n","image_path = \"scans/raw_mammogram/\"\n","annotation_path = 'scans/raw_annotation/'\n","\n","save_image_path = \"scans/preprocessed_image/\"   \n","if not os.path.exists(save_image_path):        \n","    os.mkdir(save_image_path)\n"," \n","    \n","save_mask_path = \"scans/preprocessed_mask/\"    \n","if not os.path.exists(save_mask_path):        \n","    os.mkdir(save_mask_path)\n","\n","\n","file_names = os.listdir(image_path)    \n","file_names = sorted(file_names)\n","\n","\n","for i in range(0,len(file_names)):\n","\n","    print(file_names[i])\n","    mammo = io.imread(image_path+file_names[i],0)\n","   \n","    lesion_mask = io.imread(annotation_path+file_names[i])\n","    \n","    if np.max(lesion_mask)>=0:\n","#    Extract the breast profile and crop the mammogram, breast mask and the lesion mask\n","#    Normalize the image into 16-bit\n","        breast_preprocess = Preprocess.extract_breast_profile(mammo,lesion_mask,1)\n","               \n","        mammo = breast_preprocess.image\n","        breast_mask = breast_preprocess.mask\n","        lesion_mask =breast_preprocess.lesion_mask    \n","        \n","        print ('Number of lesions: '+str(np.max(np.unique(label(lesion_mask)))))\n","        \n","#   pad the image, to ensure the aspect ratio is 1:1\n","        pad_mammo = padimages(mammo,file_names[i],1)\n","        \n","#    save the preprocessed image\n","\n","        io.imsave(save_image_path + file_names[i],pad_mammo)\n","        \n","        #if the image has more than 1 lesion, then seperate them into different masks and number them.\n","        \n","        labelim = label(lesion_mask)\n","        if np.max(labelim)>0:\n","#            if there is at least 1 lesion.\n","            for l in range(1,np.max(labelim+1)):\n","                l_mask = np.zeros(np.shape(labelim))\n","                l_mask = l_mask.astype(lesion_mask.dtype)\n","                l_mask [labelim==l] = 255\n","                num_nonzero = np.where(l_mask>0)\n","                num_nonzero = len(num_nonzero[0])\n","\n","                if num_nonzero>15:\n","                    print('A valid mask')\n","#                   Pad the mask in the same way as padding the image\n","                    pad_l_mask = padimages(l_mask,file_names[i],1)\n","                    io.imsave(save_mask_path+file_names[i][:-4]+str(l)+'.png',pad_l_mask)\n","                else:\n","                    print('Has a tiny piece of noise that is not valid for training!')\n","                    \n","        else:# if there is no lesion\n","            pad_lesion_mask = padimages(lesion_mask,file_names[i],1)\n","            io.imsave(save_mask_path+file_names[i][:-4]+str(0)+'.png',pad_lesion_mask)\n","       \n","stop = timeit.default_timer()\n","print('RunTime per image: ', (stop - start)/ len(file_names)) "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XPZDLxxlyh7A"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tuxaaw7tyh-G"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q1Nlg_mvyiBp"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cybVGyONyiL2"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ASLMBKkbyikn"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uH2fqDNbJ-Pr"},"source":["!pip install keras==2.1.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GDfJW1E5KBZf"},"source":["!pip install tensorflow==1.14.0\n","import tensorflow as tf\n","print(tf.__version__)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n9aaWLja7f4V"},"source":["!git clone https://github.com/matterport/Mask_RCNN"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w3YweWp2KBc9","executionInfo":{"status":"ok","timestamp":1616877081603,"user_tz":-360,"elapsed":888,"user":{"displayName":"Jasmin Jahan Puspo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrKQVqe7wC9jSaf9_-51xdh1f0mivy2S9SdHYt=s64","userId":"07570020587767099311"}},"outputId":"6d2a1f98-d959-4921-e680-b3c52271e053"},"source":["cd '/content/drive/MyDrive/Mammographic'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/content/drive/MyDrive/Mammographic\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"QFdFP5Q7KBfp"},"source":["import os\n","import sys\n","import json\n","import datetime\n","import numpy as np\n","import skimage.draw\n","import matplotlib.image\n","import glob\n","import scipy.misc\n","from PIL import Image\n","#import imgaug \n","from imgaug import augmenters as iaa\n","import matplotlib.pyplot as plt\n","import imageio"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XemznmFPKMsK"},"source":["# Root directory of the project\n","ROOT_DIR = os.getcwd()\n","ROOT_DIR = ROOT_DIR+\"/Mask_r_cnn\"\n","\n","MAMOGRAM_IMAGE_DIR = \"/scans/pseudo_color_image/\" #Path of the mammograms\n","MAMOGRAM_MASK_DIR = \"/scans/preprocessed_mask/\"# Path of the ground truth masks\n","\n","\n","# Import Mask RCNN\n","sys.path.append(ROOT_DIR) # To find local version of the library\n","from mrcnn.config import Config\n","from mrcnn import model as modellib, utils\n","\n","# Path to trained weights file\n","COCO_WEIGHTS_PATH = os.path.join(ROOT_DIR, \"mask_rcnn_balloon.h5\")\n","\n","# Directory to save logs and model checkpoints, if not provided\n","# through the command line argument --logs\n","DEFAULT_LOGS_DIR = os.path.join(\"/content/drive/MyDrive/Mammographic\", \"logs\")#Log directory for saving the weights\n","DEMO_SAVE_DIR = \"/scans/seg_mask/\"# path to save the segmentation masks"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WwHFrfmIKSV6"},"source":["############################################################\n","#  Configurations\n","############################################################\n","\n","\n","class MamogramConfig(Config):\n","    \"\"\"Configuration for training on the toy  dataset.\n","    Derives from the base Config class and overrides some values.\n","    \"\"\"\n","    # Give the configuration a recognizable name\n","    NAME = \"mamogram\"\n","\n","    # We use a GPU with 12GB memory, which can fit two images.\n","    # Adjust down if you use a smaller GPU.\n","    IMAGES_PER_GPU = 1\n","\n","    # Number of classes (including background)\n","    NUM_CLASSES = 1 + 1  # Background + lesion\n","\n","    # Number of training steps per epoch,set to the number of training data here\n","    STEPS_PER_EPOCH = 5\n","\n","    # Number of validation steps after each round of training\n","    VALIDATION_STEPS = 2\n","    # Resize mode: \"none\" or \"square\"\n","\n","    IMAGE_RESIZE_MODE = \"square\"\n","    IMAGE_MIN_DIM = 1024\n","    IMAGE_MAX_DIM = 1024\n","\n","    # Skip detections with < DETECTION_MIN_CONFIDENCE\n","    DETECTION_MIN_CONFIDENCE = 0.965 # alter this during testing to generate different TPR at different FPI\n","    # 0.7 0.75 0.8 0.85 0.9"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XVXJ7BV7KSZC"},"source":["############################################################\n","#  Dataset\n","############################################################\n","\n","class MamogramDataset(utils.Dataset):\n","\n","    def load_mamogram(self, subset):\n","        \"\"\"This method loads the actual image\n","        subset is either \"train\" or \"val\" depending on whether the image is part of the training or validation datasets \n","        \"\"\"\n","        # Add classes. We have only one class to add.\n","        # These are the things that will be segmented\n","        self.add_class(\"mamogram\", 1, \"lesion\")\n","\n","        # Train or validation dataset?\n","\n","        #list all the files in the directory with the mamogram images\n","        files = os.listdir(ROOT_DIR + MAMOGRAM_IMAGE_DIR + subset + \"/\")\n","        \n","        for fname in files:            \n","            self.add_image(\"mamogram\", image_id=fname, \n","                           path=ROOT_DIR + MAMOGRAM_IMAGE_DIR + subset +\"/\"+ fname, subset=subset, fname=fname)\n","\n","\n","    def load_mask(self, image_id):\n","        \"\"\"load the instance masks for an image.\n","        Returns:\n","        a tuple containing:\n","        masks: A bool array of shape [height, width, instance count] with\n","        one mask per instance.\n","        class_ids: a 1D array of class IDs of the instance masks.\n","        use dtype=np.int32\n","        \"\"\"\n","        image_info = self.image_info[image_id]\n","        info = self.image_info[image_id]\n","        fname = info['fname']\n","       \n","        files = glob.glob(ROOT_DIR + MAMOGRAM_MASK_DIR + info['subset']+\"/\" + fname[0:-4] + \"*\")\n","        \n","\n","        masks = []\n","        for i in range(0, len(files)):\n","            #print(i)\n","            data = skimage.io.imread(files[i])\n","            \n","            if data.ndim != 1:\n","                data = skimage.color.rgb2gray(data)\n","          \n","            singleMask = data\n","            if i == 0:\n","                masks = np.zeros((singleMask.shape[0], singleMask.shape[1], len(files)))\n","            masks[:,:,i] = singleMask\n","\n","        instanceMaskMap = np.array(np.ones([masks.shape[-1]], dtype=np.int32))\n","        \n","        return (masks.astype(np.bool), instanceMaskMap)\n","\n","\n","        #class_ids = np.array([self.class_names.index(s[0]) for s in fname])\n","        #return mask.astype(np.bool), class_ids.astype(np.int32)\n","         #this is VERY important: array of class ids in the order that they appear in bigdata\n","        # Return mask, and array of class IDs of each instance. Since we have\n","        # one class ID only, we return an array of 1\n","\n","    def load_image(self, image_id):\n","        \"\"\"Load the specified image and return a [H,W,3] Numpy array.\n","\t\tTaken from utils.py, any refinements we need can be done here\n","        \"\"\"\n","        # Load image\n","        image = skimage.io.imread(self.image_info[image_id]['path'])\n","        # If grayscale. Convert to RGB for consistency.\n","        if image.ndim != 3:\n","            image = skimage.color.gray2rgb(image)\n","        # If has an alpha channel, remove it for consistency\n","        if image.shape[-1] == 4:\n","            image = image[..., :3]\n","        return image\n","\n","    def image_reference(self, image_id):\n","        \"\"\"Return the path of the image.\"\"\"\n","        info = self.image_info[image_id]\n","        return info[\"path\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H6LHNTgOKSb4"},"source":["def train(model):\n","    \"\"\"Train the model.\"\"\"\n","    # Training dataset.\n","    dataset_train = MamogramDataset()\n","    dataset_train.load_mamogram(\"train\")\n","    dataset_train.prepare()\n","\n","    # Validation dataset\n","    dataset_val = MamogramDataset()\n","    dataset_val.load_mamogram(\"val\")\n","    dataset_val.prepare()\n","\n","\n","\n","\n","    # Image augmentation\n","    # http://imgaug.readthedocs.io/en/latest/source/augmenters.html\n","    aug = iaa.Sequential([\n","        iaa.OneOf([iaa.Fliplr(0.5),\n","                   iaa.Flipud(0.5),\n","                   iaa.Affine(rotate=90),\n","                   iaa.Affine(rotate=180),\n","                   iaa.Affine(rotate=270)]),\n","    ])\n","\n","    # *** This training schedule is an example. Update to your needs ***\n","    # Since we're using a very small dataset, and starting from\n","    # COCO trained weights, we don't need to train too long. Also,\n","    # no need to train all layers, just the heads should do it.\n","    print(\"Training network heads\")\n","\n","    model.train(dataset_train, dataset_val,\n","                learning_rate=config.LEARNING_RATE,\n","                epochs=3,augmentation=aug,\n","                layers='heads')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ppvWFROUKzgm"},"source":["def segment(model, imPath):\n","    \n","    image = skimage.io.imread(imPath)\n","\n","    fname = imPath.split('/')[-1]\n","    mrcnnData = model.detect([image], verbose=1)\n","       # documentation for model.detect:\n","       # \"\"\"Runs the detection pipeline.\n","\n","       # images: List of images, potentially of different sizes.\n","\n","       # Returns a list of dicts, one dict per image. The dict contains:\n","       # rois: [N, (y1, x1, y2, x2)] detection bounding boxes\n","       # class_ids: [N] int class IDs\n","       # scores: [N] float probability scores for the class IDs\n","       # masks: [H, W, N] instance binary masks\n","       # \"\"\"\n","\n","    mrcnnData = mrcnnData[0] #model.detect takes a list of images, but here we only provide one image so the output is a list with just one element\n","\n","    masks = mrcnnData['masks']\n","    for i in range(0, masks.shape[2]):\n","        #iterate through the masks\n","        maskSingle = np.squeeze(masks[:, :, i])\n","        file_name =ROOT_DIR+ DEMO_SAVE_DIR + \"demo_mask_\" + str(i) + \"_\" + fname + \"_{:%Y%m%dT%H%M%S}.png\".format(datetime.datetime.now())\n","        \n","\n","        #scipy.misc.imsave(file_name, maskSingle.astype(np.int64))\n","        plt.imshow(maskSingle.astype(np.int8))\n","        #scipy.misc.imsave(file_name, maskSingle.astype(np.int64)) \n","        matplotlib.image.imsave(file_name, maskSingle.astype(np.int64))\n"," \n","\n","\n","    print(mrcnnData)\n","    print(\"&&&&&&&&&&&: \"+str(mrcnnData['rois']))\n","    print(\"&&&&&&&&&&&: \"+str(mrcnnData['class_ids']))\n","    print(\"&&&&&&&&&&&: \"+str(mrcnnData['scores']))\n","\n","    return\n","\n","def segmentWrapper(model, directory):\n","    \"\"\"wrapper function for segment to take many images as an input, calls segment() on everything in the directory\"\"\"\n","    files = os.listdir(directory)\n","    for f in files:\n","        segment(model, directory + '/' + f)\n","\n","def overlayResult(image, mask):\n","\t\"\"\"Function to overlay segmentation mask on an image.\n","\tusage: image_var = overlayResult(image, dict['masks'] || masks_var)\n","\t\n","\timage: RGB or grayscale image [height, width, 1 || 3].\n","\tmask: segmentation mask [height, width, instance_count]\n","\t\n","\treturns resulting image.\n","\t\"\"\"\n","\t# Image is already in grayscale so we skip converting it\n","\t# May need to create 3 dimensions if single dimension image though so\n","\t# will add this as a placeholder\n","\tgray = skimage.color.gray2rgb(skimage.color.rgb2gray(image)) * 255\n","\t# Copy color pixels from the original color image where mask is set\n","\tif mask.shape[-1] > 0:\n","\t\t#collapse masks into one layer\n","\t\tmask = (np.sum(mask, -1, keepdims=True) >= 1)\n","\t\toverlay = np.where(mask, image, gray).astype(np.uint8)\n","\telse:\n","\t\toverlay = gray.astype(np.uint8)\n","\t\t\n","\treturn overlay"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"azy1i23ScKHx"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pR6ZzrpycKLV"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wM0_X7fvcKd1"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"M9lGZ6mFcKhH"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FETRtQIn3iyB"},"source":[" # Training dataset.\n","dataset_train = MamogramDataset()\n","dataset_train.load_mamogram(\"train\")\n","dataset_train.prepare()\n","\n","    # Validation dataset\n","dataset_val = MamogramDataset()\n","dataset_val.load_mamogram(\"val\")\n","dataset_val.prepare()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QzJnXQhA21uV","executionInfo":{"status":"ok","timestamp":1616869088399,"user_tz":-360,"elapsed":968,"user":{"displayName":"Jasmin Jahan Puspo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrKQVqe7wC9jSaf9_-51xdh1f0mivy2S9SdHYt=s64","userId":"07570020587767099311"}},"outputId":"baf96146-e0bf-407c-c6d3-72a530ce3323"},"source":["print(dataset_val.class_names)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["['BG', 'lesion']\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"OPkgRjNm6LMs"},"source":["config = MamogramConfig()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_MEQMtqJ9Ep"},"source":["model = modellib.MaskRCNN(mode=\"inference\", config=config,\n","                                  model_dir=DEFAULT_LOGS_DIR)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TDriWIpKWUBz"},"source":["weights_path ='/content/drive/MyDrive/Mammographic/logs/mask_rcnn_mamogram_weights.h5'"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Me0vFI68WdrY"},"source":["model.load_weights(weights_path, by_name=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mKbFhvqR5y6L"},"source":["model = modellib.MaskRCNN(mode=\"training\", config=config,model_dir=DEFAULT_LOGS_DIR)\n","model.load_weights(COCO_WEIGHTS_PATH, by_name=True, exclude=[\n","            \"mrcnn_class_logits\", \"mrcnn_bbox_fc\",\n","            \"mrcnn_bbox\", \"mrcnn_mask\"])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"zSBy2-sxKfc8","executionInfo":{"status":"ok","timestamp":1616877316039,"user_tz":-360,"elapsed":16259,"user":{"displayName":"Jasmin Jahan Puspo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrKQVqe7wC9jSaf9_-51xdh1f0mivy2S9SdHYt=s64","userId":"07570020587767099311"}},"outputId":"65f4c4da-80c0-4d1c-d0b0-dda7f0d1c342"},"source":["segment(model, '/content/drive/MyDrive/Mammographic/Mask_r_cnn/scans/pseudo_color_image/22580192_5530d5782fc89dd7_MG_R_CC_ANON.png')"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Processing 1 images\n","image                    shape: (681, 681, 3)         min:    0.00000  max:  255.00000  uint8\n","molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  150.10000  float64\n","image_metas              shape: (1, 14)               min:    0.00000  max: 1024.00000  float64\n","anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n","{'rois': array([[267, 468, 324, 530]], dtype=int32), 'class_ids': array([1], dtype=int32), 'scores': array([0.9939551], dtype=float32), 'masks': array([[[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]],\n","\n","       [[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]],\n","\n","       [[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]],\n","\n","       ...,\n","\n","       [[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]],\n","\n","       [[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]],\n","\n","       [[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]]])}\n","&&&&&&&&&&&: [[267 468 324 530]]\n","&&&&&&&&&&&: [1]\n","&&&&&&&&&&&: [0.9939551]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 432x288 with 1 Axes>"]},"metadata":{"tags":[],"needs_background":"light"}}]},{"cell_type":"code","metadata":{"id":"RDbTWwQv2mcA"},"source":["dataset = dataset_train\n","image_ids = np.random.choice(dataset.image_ids, 4)\n","for image_id in image_ids:\n","    image = dataset.load_image(image_id)\n","    masks, num_ids = dataset.load_mask(image_id)\n","    print(image,masks,num_ids)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"V3hbUTCFasou","executionInfo":{"status":"ok","timestamp":1616871716956,"user_tz":-360,"elapsed":1255469,"user":{"displayName":"Jasmin Jahan Puspo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrKQVqe7wC9jSaf9_-51xdh1f0mivy2S9SdHYt=s64","userId":"07570020587767099311"}},"outputId":"83aaa1c6-3315-4df7-bad9-b183a97030ac"},"source":["config = MamogramConfig()\n","#config.display()\n","\n","model = modellib.MaskRCNN(mode=\"training\", config=config,\n","                                  model_dir=DEFAULT_LOGS_DIR)\n","\n","weights_path = COCO_WEIGHTS_PATH\n","\n","\n","model.load_weights(weights_path, by_name=True)\n","train(model)"],"execution_count":null,"outputs":[{"output_type":"stream","text":["Training network heads\n","\n","Starting at epoch 0. LR=0.001\n","\n","Checkpoint Path: /content/drive/MyDrive/Mammographic/logs/mamogram20210327T1841/mask_rcnn_mamogram_{epoch:04d}.h5\n","Selecting layers to train\n","fpn_c5p5               (Conv2D)\n","fpn_c4p4               (Conv2D)\n","fpn_c3p3               (Conv2D)\n","fpn_c2p2               (Conv2D)\n","fpn_p5                 (Conv2D)\n","fpn_p2                 (Conv2D)\n","fpn_p3                 (Conv2D)\n","fpn_p4                 (Conv2D)\n","In model:  rpn_model\n","    rpn_conv_shared        (Conv2D)\n","    rpn_class_raw          (Conv2D)\n","    rpn_bbox_pred          (Conv2D)\n","mrcnn_mask_conv1       (TimeDistributed)\n","mrcnn_mask_bn1         (TimeDistributed)\n","mrcnn_mask_conv2       (TimeDistributed)\n","mrcnn_mask_bn2         (TimeDistributed)\n","mrcnn_class_conv1      (TimeDistributed)\n","mrcnn_class_bn1        (TimeDistributed)\n","mrcnn_mask_conv3       (TimeDistributed)\n","mrcnn_mask_bn3         (TimeDistributed)\n","mrcnn_class_conv2      (TimeDistributed)\n","mrcnn_class_bn2        (TimeDistributed)\n","mrcnn_mask_conv4       (TimeDistributed)\n","mrcnn_mask_bn4         (TimeDistributed)\n","mrcnn_bbox_fc          (TimeDistributed)\n","mrcnn_mask_deconv      (TimeDistributed)\n","mrcnn_class_logits     (TimeDistributed)\n","mrcnn_mask             (TimeDistributed)\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gradients_util.py:93: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n","  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n","/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2087: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n","  UserWarning('Using a generator with `use_multiprocessing=True`'\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch 1/3\n","4/5 [=======================>......] - ETA: 1:14 - loss: 155.5872 - rpn_class_loss: 23.3238 - rpn_bbox_loss: 132.2633 - mrcnn_class_loss: 0.0000e+00 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2330: UserWarning: Using a generator with `use_multiprocessing=True` and multiple workers may duplicate your data. Please consider using the`keras.utils.Sequence class.\n","  UserWarning('Using a generator with `use_multiprocessing=True`'\n"],"name":"stderr"},{"output_type":"stream","text":["\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r5/5 [==============================] - 432s 86s/step - loss: 154.4079 - rpn_class_loss: 22.1797 - rpn_bbox_loss: 132.2281 - mrcnn_class_loss: 0.0000e+00 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: 276.1230 - val_rpn_class_loss: 15.1062 - val_rpn_bbox_loss: 261.0169 - val_mrcnn_class_loss: 0.0000e+00 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00\n","Epoch 2/3\n","5/5 [==============================] - 342s 68s/step - loss: 210.9497 - rpn_class_loss: 8.5504 - rpn_bbox_loss: 202.3993 - mrcnn_class_loss: 0.0000e+00 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: 262.2912 - val_rpn_class_loss: 1.1692 - val_rpn_bbox_loss: 261.1220 - val_mrcnn_class_loss: 0.0000e+00 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00\n","Epoch 3/3\n","5/5 [==============================] - 340s 68s/step - loss: 247.8848 - rpn_class_loss: 0.4075 - rpn_bbox_loss: 247.4774 - mrcnn_class_loss: 2.3842e-08 - mrcnn_bbox_loss: 0.0000e+00 - mrcnn_mask_loss: 0.0000e+00 - val_loss: 269.0687 - val_rpn_class_loss: 0.0232 - val_rpn_bbox_loss: 269.0455 - val_mrcnn_class_loss: 0.0000e+00 - val_mrcnn_bbox_loss: 0.0000e+00 - val_mrcnn_mask_loss: 0.0000e+00\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HQ9ZXvxsFVbJ","executionInfo":{"status":"ok","timestamp":1616877707129,"user_tz":-360,"elapsed":32135,"user":{"displayName":"Jasmin Jahan Puspo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjrKQVqe7wC9jSaf9_-51xdh1f0mivy2S9SdHYt=s64","userId":"07570020587767099311"}},"outputId":"492f946b-126d-43f0-ba08-9a57a439426d"},"source":["!python mammo.py segment --weights=logs/mask_rcnn_mamogram_weights.h5 --image='/content/drive/MyDrive/Mammographic/Mask_r_cnn/scans/pseudo_color_image/22580192_5530d5782fc89dd7_MG_R_CC_ANON.png'"],"execution_count":null,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n","/usr/local/lib/python3.7/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n","  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n","Using TensorFlow backend.\n","\n","Configurations:\n","BACKBONE                       resnet101\n","BACKBONE_STRIDES               [4, 8, 16, 32, 64]\n","BATCH_SIZE                     1\n","BBOX_STD_DEV                   [0.1 0.1 0.2 0.2]\n","COMPUTE_BACKBONE_SHAPE         None\n","DETECTION_MAX_INSTANCES        100\n","DETECTION_MIN_CONFIDENCE       0.965\n","DETECTION_NMS_THRESHOLD        0.3\n","FPN_CLASSIF_FC_LAYERS_SIZE     1024\n","GPU_COUNT                      1\n","GRADIENT_CLIP_NORM             5.0\n","IMAGES_PER_GPU                 1\n","IMAGE_CHANNEL_COUNT            3\n","IMAGE_MAX_DIM                  1024\n","IMAGE_META_SIZE                14\n","IMAGE_MIN_DIM                  1024\n","IMAGE_MIN_SCALE                0\n","IMAGE_RESIZE_MODE              square\n","IMAGE_SHAPE                    [1024 1024    3]\n","LEARNING_MOMENTUM              0.9\n","LEARNING_RATE                  0.001\n","LOSS_WEIGHTS                   {'rpn_class_loss': 1.0, 'rpn_bbox_loss': 1.0, 'mrcnn_class_loss': 1.0, 'mrcnn_bbox_loss': 1.0, 'mrcnn_mask_loss': 1.0}\n","MASK_POOL_SIZE                 14\n","MASK_SHAPE                     [28, 28]\n","MAX_GT_INSTANCES               100\n","MEAN_PIXEL                     [123.7 116.8 103.9]\n","MINI_MASK_SHAPE                (56, 56)\n","NAME                           mamogram\n","NUM_CLASSES                    2\n","POOL_SIZE                      7\n","POST_NMS_ROIS_INFERENCE        1000\n","POST_NMS_ROIS_TRAINING         2000\n","PRE_NMS_LIMIT                  6000\n","ROI_POSITIVE_RATIO             0.33\n","RPN_ANCHOR_RATIOS              [0.5, 1, 2]\n","RPN_ANCHOR_SCALES              (32, 64, 128, 256, 512)\n","RPN_ANCHOR_STRIDE              1\n","RPN_BBOX_STD_DEV               [0.1 0.1 0.2 0.2]\n","RPN_NMS_THRESHOLD              0.7\n","RPN_TRAIN_ANCHORS_PER_IMAGE    256\n","STEPS_PER_EPOCH                5\n","TOP_DOWN_PYRAMID_SIZE          256\n","TRAIN_BN                       False\n","TRAIN_ROIS_PER_IMAGE           200\n","USE_MINI_MASK                  True\n","USE_RPN_ROIS                   True\n","VALIDATION_STEPS               2\n","WEIGHT_DECAY                   0.0001\n","\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:507: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3831: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:3655: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n","\n","WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/backend/tensorflow_backend.py:1940: The name tf.image.resize_nearest_neighbor is deprecated. Please use tf.compat.v1.image.resize_nearest_neighbor instead.\n","\n","WARNING:tensorflow:From /content/drive/MyDrive/Mammographic/Mask_r_cnn/mrcnn/model.py:341: The name tf.log is deprecated. Please use tf.math.log instead.\n","\n","WARNING:tensorflow:From /content/drive/MyDrive/Mammographic/Mask_r_cnn/mrcnn/model.py:399: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use tf.where in 2.0, which has the same broadcast rule as np.where\n","WARNING:tensorflow:From /content/drive/MyDrive/Mammographic/Mask_r_cnn/mrcnn/model.py:423: calling crop_and_resize_v1 (from tensorflow.python.ops.image_ops_impl) with box_ind is deprecated and will be removed in a future version.\n","Instructions for updating:\n","box_ind is deprecated, use box_indices instead\n","WARNING:tensorflow:From /content/drive/MyDrive/Mammographic/Mask_r_cnn/mrcnn/model.py:720: The name tf.sets.set_intersection is deprecated. Please use tf.sets.intersection instead.\n","\n","WARNING:tensorflow:From /content/drive/MyDrive/Mammographic/Mask_r_cnn/mrcnn/model.py:722: The name tf.sparse_tensor_to_dense is deprecated. Please use tf.sparse.to_dense instead.\n","\n","WARNING:tensorflow:From /content/drive/MyDrive/Mammographic/Mask_r_cnn/mrcnn/model.py:772: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n","Instructions for updating:\n","Use `tf.cast` instead.\n","Loading weights  logs/mask_rcnn_mamogram_weights.h5\n","2021-03-27 20:41:26.585652: I tensorflow/core/platform/cpu_feature_guard.cc:142] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n","2021-03-27 20:41:26.590441: I tensorflow/core/platform/profile_utils/cpu_utils.cc:94] CPU Frequency: 2299995000 Hz\n","2021-03-27 20:41:26.590724: I tensorflow/compiler/xla/service/service.cc:168] XLA service 0x55c0322ca140 executing computations on platform Host. Devices:\n","2021-03-27 20:41:26.590759: I tensorflow/compiler/xla/service/service.cc:175]   StreamExecutor device (0): <undefined>, <undefined>\n","2021-03-27 20:41:28.973056: W tensorflow/compiler/jit/mark_for_compilation_pass.cc:1412] (One-time warning): Not using XLA:CPU for cluster because envvar TF_XLA_FLAGS=--tf_xla_cpu_global_jit was not set.  If you want XLA:CPU, either set that envvar, or use experimental_jit_scope to enable XLA:CPU.  To confirm that XLA is active, pass --vmodule=xla_compilation_cache=1 (as a proper command-line flag, not via TF_XLA_FLAGS) or set the envvar XLA_FLAGS=--xla_hlo_profile.\n","Processing 1 images\n","image                    shape: (681, 681, 3)         min:    0.00000  max:  255.00000  uint8\n","molded_images            shape: (1, 1024, 1024, 3)    min: -123.70000  max:  150.10000  float64\n","image_metas              shape: (1, 14)               min:    0.00000  max: 1024.00000  float64\n","anchors                  shape: (1, 261888, 4)        min:   -0.35390  max:    1.29134  float32\n","{'rois': array([[267, 468, 324, 530]], dtype=int32), 'class_ids': array([1], dtype=int32), 'scores': array([0.9939551], dtype=float32), 'masks': array([[[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]],\n","\n","       [[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]],\n","\n","       [[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]],\n","\n","       ...,\n","\n","       [[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]],\n","\n","       [[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]],\n","\n","       [[False],\n","        [False],\n","        [False],\n","        ...,\n","        [False],\n","        [False],\n","        [False]]])}\n","&&&&&&&&&&&: [[267 468 324 530]]\n","&&&&&&&&&&&: [1]\n","&&&&&&&&&&&: [0.9939551]\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"Ebj49bDUMXS6"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MDkIxqqoIeCd"},"source":["!python mammo.py train --weights=mask_rcnn_balloon.h5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lVeFsfkSV--Q"},"source":["config = MamogramConfig()\n","#config.display()\n","\n","model = modellib.MaskRCNN(mode=\"training\", config=config,\n","                                  model_dir=DEFAULT_LOGS_DIR)\n","\n","weights_path = COCO_WEIGHTS_PATH\n","\n","\n","model.load_weights(weights_path, by_name=True)\n","train(model)"],"execution_count":null,"outputs":[]}]}